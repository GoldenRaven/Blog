* CUDA使用
** CUDA基本使用
*** 查看GPU信息
#+BEGIN_SRC python :results output
import torch
print(torch.cuda.is_available()) # 判断GPU是否可用
print(torch.cuda.device_count()) # 返回GPU个数
print(torch.cuda.get_device_name(0)) # 返回GPU名称，默认从0开始
print(torch.cuda.current_device()) # 返回当前设备索引
#+END_SRC

#+RESULTS:
*** torch.device
torch.device 表示 torch.Tensor 分配到的设备的对象。其包含一个设备类型
（cpu 或 cuda），以及可选的设备序号。
可以通过如下方式创建 torch.device 对象：
#+BEGIN_SRC python :results output
# 通过字符串
device = torch.device('cpu')
device = torch.device('cuda:1')  # 指定类型及编号。注意，代码不会检查编号是否合法
device = torch.device('cuda')    # 默认为当前设备
#+END_SRC
还可以通过设备类型加上编号，来创建 device 对象：
#+BEGIN_SRC python :results output
device = torch.device('cuda', 0)
device = torch.device('cpu', 0)
#+END_SRC

*** 配置 CUDA 访问限制
可以通过如下方式，设置当前 Python 脚本可见的 GPU.

- 在终端设置:
#+BEGIN_SRC python :results output
CUDA_VISIBLE_DEVICES=1 python my_script.py
#+END_SRC
实例:

#+BEGIN_SRC python :results output
# Environment Variable Syntax    Results
CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen
CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible
CUDA_VISIBLE_DEVICES="0,1"       Same as above, quotation marks are optional
CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked
CUDA_VISIBLE_DEVICES=""          No GPU will be visible
#+END_SRC
- 在 Python 代码中设置
#+BEGIN_SRC python :results output
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0, 2"
#+END_SRC
- 使用函数 set_device
#+BEGIN_SRC python :results output
import torch
torch.cuda.set_device(id)
#+END_SRC
官方建议使用 CUDA_VISIBLE_DEVICES，不建议使用 set_device 函数。
** 单机单卡训练
默认使用CPU训练模型，使用GPU单机单卡训练时，模型和输入必须位于同一张GPU卡上。

使用GPU方法如下：
#+BEGIN_SRC python :results output
device = torch.device('cuda: 1')
model = model.cuda(device)
model = model.to(device) # 必须指定device
#+END_SRC

** 并行训练
参考词条：[[https://github.com/tczhangzhi/pytorch-distributed]]

*** 简单的方便 ~nn.DataParallel~ [不推荐使用，distributed更高效！]
需要用 ~torch.nn.DataParallel~ 来包装模型，包装模型时需要设置一些参数，包括：
参与训练的GPU有哪些， ~device_ids~ ;用于汇总梯度的GPU是哪个， ~output_device~ 。
~nn.DataParallel~ 会自动将数据切分load到相应GPU，进行正向传播，并计算梯度，所以
batch_size应设置为 ~单个GPU上的batch_size * 使用的GPU个数~ ：
#+BEGIN_SRC python
import torch.nn as nn

model = MyNet()
model = model.cuda() # 需要这一步吗？
net = nn.DataParallel(
                      model, # 要被并行化的模块
                      device_ids=[0, 1, 2], # 整数列表或torch.device类型，默认值为所有的CUDA device
                      output_device=0 # 输出的device位置，默认值为device_ids[0]
                      )
output = net(input_var) # input_var可以在任何device上，包括CPU上。新的变动？
#+END_SRC

#+BEGIN_EXAMPLE
注意：
  1. 是否需要手动将输入数据移动到GPU上？
  2. 是否需要 model.to(device)？
#+END_EXAMPLE

由单进程控制多个GPU。
*** ~torch.distributed~ 并行
有了distributed之后，我们可以只写一份代码，torch会自动将其分配给n个进程，分别在n个进程
上运行。

pytorch为我们提供了torch.distributed.lanch启动器，用于在命令行分布式的执行Python
文件。可以这样获得当前进程（也即GPU的）的index:
#+BEGIN_SRC python
parser = ArgumentParse()
parser.add_argument('__local_rank', default=-1, type=int,
                    help='node rank for distributed training')
args = parse.parse_args()

print(args.local_rank)
#+END_SRC

然后使用 ~init_process_group~ 设置GPU通信所作用的后端和端口。初始化进程组主要有三种
方法：
- 显式指定 ~store~  ~rank~  ~word_size~
- 指定 ~init_method~ ，来告诉进程如何去发现其他并行进程（ ~init_method~ 是个URL
  字符串），并且指定 ~rank~ 和 ~word_size~ 或将它们写入URL字符串中。
- 如果以上两种都没有指定，则假设 ~init_method~ 为 ~env://~
#+BEGIN_SRC python
import torch.distributed as dist
parser.add_argument('--dist-url', default='tcp://127.0.0.1:3456', type=str, help='')
dist.init_process_group(backend='nccl', # nccl的GPU支持目前是最好的，推荐使用
                        init_method=args.dist_url, # 字符串
                        world_size= ， # 整数，可选，任务的进程数。如果store给定，则必须指定
                        rank= , # 当前进程的rank, 如果store给定，则必须指定
                        store=  #
                       )
#+END_SRC

然后，使用DistributedSampler对数据集进行划分
#+BEGIN_SRC python
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)
#+END_SRC

然后，使用 DistributedDataParallel 包装模型
#+BEGIN_SRC python
model = torch.nn.parallel.DistributedDataParallel(model,
                                                  device_ids=[args.local_rank])
#+END_SRC

最后把数据和模型加载到当前进程使用的GPU中，进行向前向后传播：
#+BEGIN_SRC python
torch.cuda.set_device(args.local_rank)
model.cuda()

for epoch in range(100):
   for batch_idx, (data, target) in enumerate(train_loader):
      images = images.cuda(non_blocking=True)
      target = target.cuda(non_blocking=True)
      # ...
      output = model(images)
      loss = criterion(output, target)
      # ...
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
#+END_SRC

在使用时，调用 torch.distributed.launch 启动器启动：
#+BEGIN_SRC shell
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py
#+END_SRC
