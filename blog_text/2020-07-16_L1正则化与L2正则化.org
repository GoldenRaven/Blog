* L1正则化与L2正则化
L1正则化在损失函数上加L1惩罚项， \alpha \sum^{n}_{i=1}|\Theta_{i}|，
对应于Lasso正则项。
L2正则化在损失函数上加L2惩罚项， \alpha/2 \sum^{n}_{i=1}\Theta^{2}_{i}。
它们都是正则化，也就是说都可以对模型进行约束，以减少过拟合。

** 函数极小值角度
L1正则化因为使用了绝对值
函数，所以当正则化参数大时，会使绝对值函数的贡献变大，其结果是，函数最小值出现在原点
位置，也就是\theta=0处。也就是说，L1正则化倾向于使模型参数变成0，最终得到一个稀疏
解（解向量中许多元素为0），这也就对特征进行了筛选，舍弃了那些权重为0的向量。

相比之下L2正则化更趋向于得到平滑的权重系数，得到整体上较小的权重参数。
** 概率论角度
L1正则化对应于log Laplace分布。此分布在零点值很大，其余地方值很小。对模型
加L1正则化相当于在模型上加入了先验假设，数据是Laplace分布的，自然会得到稀疏解。

而L2正则化对应于log 高斯分布，相当于引入了高斯分布先验假设，得到的解会是整体小的
权重。
** 优化角度
L1正则化是菱形，与原等高线的相切点会移动到原点；L2正则化为圆形，会得到较小的值，
但不会为0.
