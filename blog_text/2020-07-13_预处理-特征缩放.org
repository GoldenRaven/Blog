* 特征缩放方法：标准化(standardization)和归一化(normalization)
都是对数据进行缩放的方式，将原始的一列数据转换到某个范围，或者某种形态。
例如，一个特征完全以千克为单位，而另一个特征以克为单位，另一个以升为单位，
依此类推。当这些特征在呈现的量级方面差异很大时，我们如何使用这些特征？
** 为什么要对数据进行特征缩放？
一些机器学习算法对特征缩放敏感，另外一些则不受其影响。
- 使用梯度下降作为优化技术的机器学习算法，如线性回归、逻辑回归、神经网络
  等，需要对数据进行缩放。梯度下降公式：
  
  \Theta_{j} := \Theta_{j} - \alpha/m \sum^{m}_{i=1} (h_{\Theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)}
  
  可以看到x_{j}^{(i)}的存在会影响梯度下降的步长。保持所有特征的尺度相同，
  可以保证梯度下降快速收敛到极小值。

- 基于距离的算法，如KNN，K-means, SVM受特征范围的影响最大，因为它们
  以数据点的距离来衡量相似性。尺度大的特征在算法中将有更大的权重，所以
  要先进行特征缩放。

决策树模型对特征的尺度不敏感，它仅基于单个特征拆分节点，不受其他特征的影响。
** 归一化（Normalization）
归一化是一种缩放技术，对值进行移位和重新缩放，以使它们最终在0到1之间变化。
这也称为“最小-最大”缩放。归一化公式：

X' = (X - X_{min}) / (X_{max} - X_{min})

归一化对数据的缩放只与最大值和最小值有关，与其他数据点无关。
** 标准化（Standardization）
标准化将数据变成均值为0，并且数据的标准偏差为1。标准化公式：

X' = (X-\mu) / \sigma

\mu 和\sigma 分别是数据的均值与标准差。注意：此时缩放后的值不限于特定范围。
** 如何选择归一化与标准化？
- 当知道数据的分布不遵循高斯分布时，可以使用归一化。这在不假设数据有任何分布的算法
  （例如KNN和神经网络）中很有用。
- 在数据遵循高斯分布的情况下，标准化可能会有所帮助。但是，这不一定是正确的。
  而且，与归一化不同，标准化没有边界范围。因此，即使您的数据中有异常值，
  它们也不会受到标准化的影响。

归根结底，选择使用归一化还是标准化将取决于问题和所使用的机器学习算法。
没有硬性规定可以告诉你何时对数据进行归一化或标准化。你始终可以通过用原始、
归一化和标准化的数据将模型拟合，并比较性能以获得最佳结果。

通常的作法是：在训练集上拟合特征缩放器，再将拟合得到的缩放器应用于测试集，
这样可以避免发生数据泄漏。而且，通常不需要缩放目标变量（标签）。
** Python实现
#+BEGIN_SRC python :results output
from sklearn.preprocessing import MinMaxScaler # 归一化
from sklearn.preprocessing import StandardScaler # 标准化
from sklearn.preprocessing import scale # 标准化
#+END_SRC

#+RESULTS:
: x

scale()函数直接对数据进行缩放，不能保存缩放的参数，如均值、方差等，
但它可以沿任意一个轴对数据集标准化，如对行进行。
其他两个方法可以先在训练集上fit(), 再在测试集上transform()。
